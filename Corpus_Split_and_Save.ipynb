{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting and Saving the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is planed to be converted to `py` script afterwards.  \n",
    "It includes two functions:  \n",
    "1. Splitting the corpus into 3: training, validating and testing\n",
    "2. Save the datasets to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three steps: 1. load the corpus, 2. take a peek of the corpora (head & tail, most and least frequent words), 3. split the corpus in three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(corpus_file):\n",
    "    data = []\n",
    "    with open(corpus_file, encoding='utf+8') as in_file:\n",
    "        for line in in_file:\n",
    "            parts = line.strip().split(' ', 3) #the corpus data follows the format of catrgory-sentiment-ID-text, so I split each line into 4\n",
    "            \n",
    "            if len(parts) == 4: #check if all 4 parts are complete\n",
    "                \n",
    "                category, sentiment, id_, text = parts #hope this would work, I'm lost a bit here\n",
    "                \n",
    "                #data tagging\n",
    "                data.append({\n",
    "                    'category' : category,\n",
    "                    'sentiment' : sentiment,\n",
    "                    'id_' : id_,\n",
    "                    'text' : text \n",
    "                })\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try loading the corpus\n",
    "corpus_file = '/Users/hongxuzhou/LfD/Week1/reviews-LfD.txt' #Dont't forget to change the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     category sentiment      id_  \\\n",
      "0       music       neg  575.txt   \n",
      "1         dvd       neg  391.txt   \n",
      "2      health       neg  848.txt   \n",
      "3      camera       pos  577.txt   \n",
      "4         dvd       neg  400.txt   \n",
      "...       ...       ...      ...   \n",
      "5995   health       neg  309.txt   \n",
      "5996   health       pos  101.txt   \n",
      "5997    music       pos  671.txt   \n",
      "5998      dvd       neg  235.txt   \n",
      "5999   camera       neg   96.txt   \n",
      "\n",
      "                                                   text  \n",
      "0     the cd came as promised and in the condition p...  \n",
      "1     this was a very annoying and boring flick that...  \n",
      "2     the braun ls-5550 silk&soft bodyshave recharge...  \n",
      "3     when it comes to buying camcorders , i persona...  \n",
      "4     i had high hopes for this series when i starte...  \n",
      "...                                                 ...  \n",
      "5995  i like the idea , but the slippers just are n'...  \n",
      "5996  i eat one of these twice a week before i play ...  \n",
      "5997  i get the sense that the fleetwoods'body of re...  \n",
      "5998  if i 'm not mistaken , the only difference bet...  \n",
      "5999  i agree with m. arse with the 2star rating . i...  \n",
      "\n",
      "[6000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df = read_corpus(corpus_file)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the corpus  \n",
    "The corpus is tagged and re-ordered, so stratified sampling will be used when splitting the corpus in training, validating and testing sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I resorted to ChatGPT when doing this part, extra careful!!!\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_corpus(df, train_size=0.7, val_size=0.15, test_size=0.15, stratify_col='sentiment'):\n",
    "    # First, split off the test set\n",
    "    train_val, test = train_test_split(df, test_size=test_size, stratify=df[stratify_col], random_state=42)\n",
    "    \n",
    "    # Then, split the remaining data into train and validation sets\n",
    "    relative_val_size = val_size / (train_size + val_size) #The total amount of the data has changed after the 1st split, so the ratio needs to be recalculated\n",
    "    train, val = train_test_split(train_val, test_size=relative_val_size, stratify=train_val[stratify_col], random_state=42)\n",
    "    \n",
    "    return train, val, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the corpus\n",
    "train, val, test = split_corpus(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the corpus\n",
    "**Update**: [06/09/2024] Changed the file form from `.csv` to `.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_datasets(train, val, test):\n",
    "    #if the output dir doesn't exist, create one\n",
    "    if not os.path.exists('datasets'):\n",
    "        os.mkdir('datasets')\n",
    "    #To create a dictionary for the datasets and file names\n",
    "    datasets = {'train.txt': train, 'val.txt' : val, 'test.txt' : test}\n",
    "    \n",
    "    #To save the datasets one by one, use tab as the delimiter\n",
    "    for filename, df in datasets.items():\n",
    "        df.to_csv(f'datasets/{filename}', sep = '\\t', index = False, header = False)\n",
    "        print(f'{filename} saved.') #To check if each file is saved as expected\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.txt saved.\n",
      "val.txt saved.\n",
      "test.txt saved.\n"
     ]
    }
   ],
   "source": [
    "save_datasets(train, val, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
